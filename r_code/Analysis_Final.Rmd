---
title: "Analysis of "
date: "June 23, 2018"
output:
  pdf_document: 
    keep_tex: true
  html_document:
    df_print: paged
header-includes:
- \usepackage{amsmath}
- \usepackage{mathtools}
- \usepackage{float}
- \usepackage{xcolor,pifont}
- \newcommand{\cmark}{\Large\textcolor{green}{\ding{52}}}
- \newcommand{\xmark}{\Large\textcolor{red}{\ding{55}}}



---





## Loading required libraries

The following code loads useful libraries that aren't included in base R.  The
of these libraries come from the "tidyverse" including dplyr for manipulating
dataframes, tidyr for making data tidy, knitr for creating reproducible documents
ggplot2 for plots, maps for help with geographic plots, RColorBrewer...

```{r echo=FALSE, tidy=TRUE}

require(dplyr)
require(tidyr)
require(knitr)
require(ggplot2)
require(maps)
require(RColorBrewer)
require(summarytools)
require(magrittr) 
#automatically set working directory to the directory containing this R script

opts_chunk$set(results='asis') #table format option



```




##Import Breweries Data

In this section we load and begin cleaning the data in order to aid our
exploratory analysis.  Column names are set to lowercase for ease of reading and
we begin to summarize the data.

```{r}


#import breweries data
breweries_data <- read.csv("../data/Breweries.csv", header=TRUE)


colnames(breweries_data) %<>% tolower #lower case colnames

breweries_data %<>% rename(brewery_id = brew_id) #rename

#summary of breweries raw data
brewery_summary_raw <- select(breweries_data, state, brewery_id) %>% #select columns
                   dplyr::group_by(state) %>% #group by
                   dplyr::summarize_all(funs(n_distinct(.))) 




kable(brewery_summary_raw, digits = 0)




# #TODO: Fix chart
# ggplot(brewery_summary_raw, aes(x=state, color=brewery_id)) +
#     geom_histogram(aes(y=..density..,fill = brewery_id, alpha=0.5), binwidth = 5, stat="count") +
#     #stat_count(aes("identity")) +
#     geom_density(alpha=.2, fill="black", color="black") +
#     #facet_grid(. ~ gender) +
#     theme_bw() +
#     theme(legend.position="bottom")
#     #theme(legend.position="none")
#     #theme(text = element_text(size=10), axis.text.x = element_text(angle=90, hjust=1)) 
# 

```




## Clean Breweries Data



Before we can confidently proceed with our analysis it's important to ensure
we have scrubbed the data, removed duplicates, and decide how we will deal with
errors and missing values.

We start this process by removing punctuation and whitespace from columns.  Humans
are fallible and typos are easy to make.  Without knowing the origin of the data
in the files provided, its prudent to assume that mistakes have been made and 
take measures to correct them.

Remvoing punctuation allows us to mitigate the possibility of commas being
erroneously typed as periods.  "Detroit, MI", for example, would be identified
as a different city than "Detroit. MI"  Removing punctuation resolves this issue.  
Both city/state combinations simply become "Detroit MI."

Likewise, it's helpful to remove whitespace.  Although whitespace can appear
"invisible" to the human eye, computers can "see" this space as if it were a
number or a letter.

We use the apply function to make these changes to every row in the dataframe.

Removing duplicates is more of a challenge.  Before we can remove duplicates we
need to confirm whether or not two rows are the same.  We identify duplicates by
creating a unique key for each brewery that's a combination of the brewery ID,
city, and state.  

De-duplicating in this case is a multi-step process.  We start by identifying
brewery ids that show up more than once which indicate possible duplicates.  
Further investigation determines whether or not they are actually duplicates.

In addition to removing identifying and removing duplicates programatically, we
also need to correct a few entries manually.  There are some entries that are 
clearly mis-spelled and need to be addressed.

Once potential duplicates are identified and assigned temporary keys, they are
evaluated apart from the main dataset and returned to the main dataset once 
duplicates have been removed.




```{r}
#TODO: Breakup chunk


# remove punctionation from all columns and trim whitespace
breweries_data <- as.data.frame(
                      apply(breweries_data #data set
                            , 2 #apply function column-wise
                            , function(x) trimws(gsub('[[:punct:] ]+',' ',x))) #anonymous function to remove punctuation and trim whitespace
                            , stringsAsFactors = FALSE)  #do not implicitly convert strings to factors


breweries_data$name <- as.factor(breweries_data$name) # convert Name column to factor
breweries_data$brewery_id <- as.integer(breweries_data$brewery_id) # convert Brew_ID to integer

# confirm Brew_ID + City + State is a unique key
breweries_summary <- 
  select(breweries_data, brewery_id, city, state, name) %>%
  group_by(name) %>%
  summarize_all(funs(
    count = n_distinct(brewery_id, city, state))) %>%
  select(name, brewery_id_count) %>% # select only Name and Brew_ID_count columns
  arrange(desc(brewery_id_count)) # sort by Brew_ID_count desc
 



# capture potential duplicates
breweries_dups <- filter(breweries_summary, brewery_id_count > 1) # if Brew_ID_count > 1 then there is a potential duplicate on that Brew_ID

# rejoin potential dups to original dataset
breweries_dups <- select(breweries_dups %>% inner_join(breweries_data, by="name"), -ends_with("_count"))


# Fix Errors #

# Fix Brew_ID=378, change City(Menominee -> Menominie) 
breweries_dups <- breweries_dups %>%
     mutate(City=replace(city, brewery_id==378, "Menominie")) %>%
     as.data.frame()

#TODO: Fix SKs

# Fix Brew_ID=96, change State(MA -> MI)
breweries_dups <- breweries_dups %>%
     mutate(State=replace(state, brewery_id==96, "MI")) %>%
     as.data.frame()

#capture known duplicates
breweries_dups <- breweries_dups %>%
                  group_by(name, city, state) %>%
                  filter(n()>1)


#create surrogate key for duplicates
breweries_sk <- breweries_dups %>%
                    group_by(name, city, state) %>%
                    summarize_all(funs(
                        brew_sk = (sum(brewery_id)*sum(brewery_id)),
                        count = n()
                        )) %>% #end summarize_all
                    ungroup() %>%
                    right_join(breweries_dups, by = c("name", "city", "state")) %>% # rejoin to dupes by name, city, state
                    select(brewery_id, brewery_id_brew_sk)
  



breweries_data$brewery_id[(breweries_data$brewery_id %in% breweries_sk$brewery_id)] <- breweries_sk$brewery_id # update Brew_ID in original dataset 



breweries_clean <- distinct(breweries_data, brewery_id, .keep_all = TRUE) %>% rename(brewery_name = name) # select distinct breweries according to the unique composite key and rename



```





## Clean Beer Data



A similar process is used to remove duplicates from the Beers dataset.



```{r}

beer_data <- read.csv("../data/Beers.csv", header=TRUE)


head(beer_data)

colnames(beer_data) %<>% tolower #lower case colnames


beer_data$brewery_id[(beer_data$brewery_id %in% breweries_sk$brewery_id)]  <- breweries_sk$brewery_id_brew_sk # update brewery_ids from brewery_sk data


beer_clean <- distinct(beer_data)#%>% rename(Brew_ID = Brewery_id, Beer_Name = Name) # 

# kable(as.data.frame(summarytools::descr(beer_clean)),digits = 2)



```





## Question 1





To determine the number breweries in each state we simply count the number of times
each state appears in the table.  



```{r}




#TODO: break up chunk

state_ll <- read.csv("../data/state_coords.csv") %>% 
                    mutate(State = toupper(State)) %>% 
                    rename(name = State, lat_center = Latitude, lon_center = Longitude) #%>% select(-Latitude, -Longitude)

states <- map_data("state") %>%
          mutate(region = toupper(region)) %>%
          rename(name=region) %>%
          select(long, lat, name, group)
 
# states %>% group_by(name) %>%
#             summarise_all(funs(n=n()))
# 
#        
states <- states %>%          
          left_join(
            states %>%
            group_by(name) %>%
            summarise_all(funs(n=n())) %>%
            select(name, group_n) %>%
            distinct(name, .keep_all = TRUE)
          )
          
          



breweries_by_state <- select(breweries_clean, brewery_id, state) %>%
  group_by(state) %>%
  summarise_all(funs(brewery_count = n()))  %>%
  left_join(state_ll, by=c("state" = "Abbr"))


# state_ll %>%
#   inner_join(states)



kable(as.data.frame(summarytools::descr(breweries_by_state, transpose = TRUE)),digits = 2)

# freq(breweries_clean$state, order = "freq")



```






```{r}
#map of breweries by state

#one to many join of breweries by state
breweries_geo <- breweries_by_state %>%
                  inner_join(states, by = c("name" = "name"))

# the color scales REFUSE to work
ggplot(data = breweries_geo, aes(group = state, stat="identity")) +
  geom_polygon(aes(x = long, 
                   y = lat, 
                   group=group, 
                   fill=brewery_count), 
               color = "black") + 
  geom_text(data = (breweries_by_state %>% 
                    filter(!(state %in% c("AK", "DC", "HI")))), #filter to continental 50 states
            aes(x = lon_center, 
                y = lat_center, 
                label = as.character(brewery_count), 
                size = 7)
            ) +
  coord_fixed(1.3) 



# TODO: Overlay frequencies on states
# TODO: fix color scale
# TODO: high level summary table

```


## Question 2



In data science, like in life, sometimes less is more.  Instead of maintaining
separate tables for breweries and beers, it's helpful to merge the two datasets
into a single combined dataset.  We do this by joining the two tables by the
Brew_ID variable.



```{r}
# merge beer and breweries
merged_data <- breweries_clean %>%
               full_join(beer_clean, by="brewery_id")


#TODO: Plot -> brews by brewery

```




## Question 3



Sometimes data are not available.  This analysis is no exception.  To better
understand how our analysis could be impacted by missing values we first have to
identify and county them.

Below is a count missing values by variable.


```{r}

# Number of nulls in each column
merged_data %>%
  select_if(function(x) any(is.na(x))) %>% 
  summarise_all(funs(sum(is.na(.))))


merged_data %>% filter(state == 'CA')

#TODO: add plot?

```


## Question 4



Computing the median is straightforward.  We simply...



```{r fig.height = 12, fig.width = 12} 


#TODO: Make bar plot pretty

merged_by_state <- select(merged_data, state, abv, ibu) %>%
                   group_by(state) %>%
                   summarise_all(median, na.rm = TRUE)#funs(median(!is.na(.)))) #TODO: Double check this is calculating correctly

merged_by_state$state <- as.factor(merged_by_state$state)  

#kable(as.data.frame(summarytools::descr(beer_clean)),digits = 2)


#TODO: facet by state

ggplot(merged_by_state, aes(x=state, y=abv)) +
  geom_bar(stat = "identity", position = "dodge") +
  ylim(0, .075) +
  #facet_grid(state ~ .) +
  theme(text = element_text(size=10),
        axis.text.x = element_text(angle=90, hjust=1)) 

ggplot(merged_by_state, aes(x=state, y=ibu)) +
  geom_bar(stat = "identity", position = "dodge") +
  #ylim(0, .075) +
  #facet_grid(state ~ .) +
  theme(text = element_text(size=10),
        axis.text.x = element_text(angle=90, hjust=1)) 


```



## Question 5



Before you can "push the limits" you have to know what the limits are.  We want
to determine which state has the most alcoholic beer and which state has the most
bitter beer.

This is relatively simple.  We can determine this visually using boxplots and
confirm programmatically by sorting the tables in descending order based on the
values of interest.


```{r}

  

ggplot(merged_data, aes(x=state , y=abv)) +  #TODO: Move to Appendix
  geom_boxplot() +
  #ylim(0, .075) +
  theme(text = element_text(size=10),
        axis.text.x = element_text(angle=90, hjust=1)) 
 
ggplot(merged_data, aes(x=state , y=ibu)) +  #TODO: Move to Appendix
  geom_boxplot() +
  #ylim(0, .075) +
  theme(text = element_text(size=10),
        axis.text.x = element_text(angle=90, hjust=1)) 
 
max_abv <-  (select(merged_data, state, abv) %>%
                   group_by(state) %>%
                   #filter(ABV == max(ABV)) %>%
                   arrange(desc(abv))  %>% #sort by ABV
                   filter(row_number() == 1))[1,] #get first row
          
max_abv


max_ibu <-  (select(merged_data, state, ibu) %>%
                   group_by(state) %>%
                   #filter(ABV == max(ABV)) %>%
                   arrange(desc(ibu))  %>% #sort by ABV
                   filter(row_number() == 1))[1,] #get first row
          
max_ibu



```


## Question 6



Summarizing the statistics for ABV can be accomplished in a signle commande.


```{r}

#summaryize ABV

# tidy_summary <- tidy(summary(merged_data$ABV)) #For some reason this line wont knit


abv_stats <- as.data.frame(t(summary(merged_data$abv))) %>% #summarize and transpose
             rename("abv"=Freq, Statistic=Var2) %>%
             select(Statistic, abv)


abv_stats$abv <- round(abv_stats$abv, digits = 3)
  

abv_stats #TODO: Add IQR, stdev    #TODO: Compare to quinton's summary




```


## Question 7



To determine the relationship between ABV and IBU it's helpful to see all values
for both variables at the same time.  This is most easily accomplished using
a scatterplot.

With ABV on one axis and IBU on the other we start to see what appears to be a 
linear correlation between the values.

Adding a trendline allows us to determine a formula that specifies this correlation.


```{r fig.width = 10, fig.height = 10}


#A distinct list of beer styles, as classified by a unique style Name, IBU, ABV, and Ounces values.

styles <- beer_clean %>% 
              distinct(Beer_ID, Style, IBU, ABV, Ounces) %>% 
              arrange(Style) %>% 
              na.omit(IBU, ABV)

```


* Plot ABV v. IBU

```{r fig.width= 10, fig.height = 10}

ggplot(styles, aes(x=ABV, y=IBU)) +
  geom_point() +
  scale_colour_brewer() +
  ggtitle("ABV v. IBU") +
  theme(plot.title = element_text(hjust = 0.5))

```

# Analysis using Spearman Rank-Order Correlation

    + More info on Spearman test: https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php


* Problem: We wish to test if there is a monotonic association between the alochol by volume (ABV) and international bitterness unit (IBU) rating of beers selected from domestic craft breweries.

* Hypotheses:
  + H~o~: $\rho= 0$
  + H~A~: $\rho\neq 0$


* Assumptions:
    + Continuity of data: \cmark
    + Paired observations: \cmark
    + Data has linear relationship: \cmark
    + No significant outliers: \xmark
    + Normality: \xmark
    
```{r}

#Scatter plot of ABV v IBU
ggplot(styles, aes(x=ABV, y=IBU)) +
  geom_point() +
  scale_colour_brewer() +
  geom_smooth(method = "lm", se = FALSE, color="red") +
  theme(legend.position="none") +
  ggtitle("ABV v. IBU") +
  theme_minimal()  +
  theme(plot.title = element_text(hjust = 0.5))
```



  + Due to the lack of normality of the IBU variable and the presence of outliers in both variables, we will use the Spearman Rank-Correlation test as an alternative to the preferred Pearson Correlation.

  




```{r fig.width= 10, fig.height=10}

# QQ Plots of IBU and ABV


#calulate line fit
y <- quantile((styles$IBU %>% na.omit()), c(0.25, 0.75))
x <- qnorm(c(0.25, 0.75))
slope <- diff(y)/diff(x)
y_int <- y[1] - slope * x[1]

qq_ibu <- ggplot(styles, aes(sample = styles$IBU)) + 
              geom_qq(shape = 16, size = 2, alpha = 0.5) +
              geom_abline(slope = slope, intercept = y_int, colour ='red', size = 1) +
              ggtitle("QQ-plot of IBU") +
              theme_minimal()  +
              theme(plot.title = element_text(hjust = 0.5))


#calulate line fit
y <- quantile((styles$ABV %>% na.omit()), c(0.25, 0.75))
x <- qnorm(c(0.25, 0.75))
slope <- diff(y)/diff(x)
y_int <- y[1] - slope * x[1]  

qq_abv <- ggplot(styles, aes(sample = styles$ABV)) + 
            geom_qq(shape = 16, size = 2, alpha = 0.5) +
            geom_abline(slope = slope, intercept = y_int, colour ='red', size = 1) +
            ggtitle("QQ-plot of ABV") +
            theme_minimal() +
            theme(plot.title = element_text(hjust = 0.5))



# Histograms
hist_ibu <- ggplot(styles) +
              geom_histogram(aes(x=IBU)) +
              theme(text = element_text(size=10),
                  axis.text.x = element_text(angle=90, hjust=1)) 

hist_abv <- ggplot(styles) +
              geom_histogram(aes(x=ABV)) +
              theme(text = element_text(size=10),
                  axis.text.x = element_text(angle=90, hjust=1)) 



grid.arrange(qq_abv, qq_ibu)


```





## Appendex



#### Session Info

```{r}
sessionInfo()
```




